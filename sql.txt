import pandas as pd
import cx_Oracle
from typing import Dict, Optional


def oracle_to_pandas(connection: cx_Oracle.Connection, schema_name: str, table_name: str) -> pd.DataFrame:
    """
    Read an Oracle table and preserve its schema in a pandas DataFrame using pure cx_Oracle.
    
    Args:
        connection: A cx_Oracle connection object
        schema_name: Oracle schema name
        table_name: Oracle table name
        
    Returns:
        pd.DataFrame: Pandas DataFrame with data from the Oracle table with preserved schema
    """
    # Setup output type handler for proper date handling
    def output_type_handler(cursor, name, defaultType, size, precision, scale):
        if defaultType == cx_Oracle.DB_TYPE_DATE or defaultType == cx_Oracle.DB_TYPE_TIMESTAMP:
            return cursor.var(cx_Oracle.DB_TYPE_TIMESTAMP, arraysize=cursor.arraysize)
        return None
    
    connection.outputtypehandler = output_type_handler
    
    # Create cursor
    cursor = connection.cursor()
    
    try:
        # Get column metadata
        columns_query = """
        SELECT column_name, data_type
        FROM all_tab_columns
        WHERE owner = :schema AND table_name = :table
        ORDER BY column_id
        """
        
        cursor.execute(columns_query, {"schema": schema_name.upper(), "table": table_name.upper()})
        columns = cursor.fetchall()
        
        if not columns:
            # If no columns found, maybe try different case
            cursor.execute(columns_query, {"schema": schema_name, "table": table_name})
            columns = cursor.fetchall()
            
            if not columns:
                raise ValueError(f"Table {schema_name}.{table_name} not found. Check schema and table names.")
        
        # Extract column names
        column_names = [col[0] for col in columns]
        
        # Try different query formats - start with the simplest approach
        query_formats = [
            f'SELECT * FROM {schema_name}.{table_name}',
            f'SELECT * FROM "{schema_name}"."{table_name}"',
            f'SELECT * FROM {schema_name}."{table_name}"',
            f'SELECT * FROM "{schema_name}".{table_name}',
            f'SELECT * FROM {schema_name.upper()}.{table_name.upper()}',
            f'SELECT * FROM {schema_name.lower()}.{table_name.lower()}'
        ]
        
        # Try each query format until one works
        success = False
        last_error = None
        
        for query in query_formats:
            try:
                cursor.execute(query)
                success = True
                break
            except cx_Oracle.DatabaseError as e:
                last_error = e
                continue
        
        if not success:
            # If all failed, raise the last error
            raise last_error
        
        # Fetch all rows
        rows = cursor.fetchall()
        
        # Create DataFrame
        df = pd.DataFrame(rows, columns=column_names)
        
        return df
    
    finally:
        # Close cursor
        cursor.close()


def get_table_schema(connection: cx_Oracle.Connection, schema_name: str, table_name: str) -> Dict:
    """
    Get schema information for a specific Oracle table.
    
    Args:
        connection: A cx_Oracle connection object
        schema_name: Oracle schema name
        table_name: Oracle table name
        
    Returns:
        Dict: Dictionary with column information
    """
    cursor = connection.cursor()
    
    try:
        query = """
        SELECT column_name, data_type, data_length, data_precision, data_scale, nullable
        FROM all_tab_columns
        WHERE owner = :schema AND table_name = :table
        ORDER BY column_id
        """
        
        cursor.execute(query, {"schema": schema_name.upper(), "table": table_name.upper()})
        columns = cursor.fetchall()
        
        if not columns:
            # Try with different case
            cursor.execute(query, {"schema": schema_name, "table": table_name})
            columns = cursor.fetchall()
            
            if not columns:
                raise ValueError(f"Table {schema_name}.{table_name} not found. Check schema and table names.")
        
        schema = {}
        for col in columns:
            col_name = col[0]
            col_type = col[1]
            length = col[2]
            precision = col[3]
            scale = col[4]
            nullable = col[5]
            
            type_desc = col_type
            if col_type == 'NUMBER' and precision is not None:
                if scale == 0 or scale is None:
                    type_desc = f'NUMBER({precision})'
                else:
                    type_desc = f'NUMBER({precision},{scale})'
            elif col_type in ('VARCHAR2', 'NVARCHAR2', 'CHAR', 'NCHAR'):
                type_desc = f'{col_type}({length})'
            
            schema[col_name] = {
                'type': type_desc,
                'nullable': nullable == 'Y'
            }
        
        return schema
    
    finally:
        cursor.close()

COPY your_table_name
FROM 's3://your-bucket-name/path/to/your-file.dat'
IAM_ROLE 'arn:aws:iam::your-account-id:role/your-redshift-role'
DELIMITER '|'
ENCODING 'LATIN1'
IGNOREHEADER 1
REMOVEQUOTES
TRIM
DATEFORMAT 'DD-MON-YY';



--------

-- Step 1: Create a temporary table with the same structure as your source data
CREATE TEMP TABLE temp_import (
    column1 VARCHAR(100),
    column2 INTEGER,
    column3 DATE,
    -- Add all columns from your source data
    -- Use appropriate data types
);

-- Step 2: COPY data from S3 into the temporary table
COPY temp_import
FROM 's3://your-bucket-name/path/to/your-file.dat'
IAM_ROLE 'arn:aws:iam::your-account-id:role/your-redshift-role'
DELIMITER '|'
ENCODING 'LATIN1'
IGNOREHEADER 1
REMOVEQUOTES
TRIM
DATEFORMAT 'DD-MON-YY';

-- Step 3: Insert from temporary table into stage table with additional columns
INSERT INTO stage_table (
    source_column1,
    source_column2,
    source_column3,
    -- Original columns from source
    
    duplicated_column,  -- Column with same values as another column
    null_column,        -- Column with all NULLs
    audit_created_date, -- Audit column (NULL for now)
    constant_column,    -- Column with constant value
    another_null_column -- Another NULL column
)
SELECT
    column1,
    column2,
    column3,
    -- Original columns from source
    
    column2,           -- Duplicating values from column2
    NULL,              -- Adding NULL column
    NULL,              -- audit_created_date as NULL
    'CONSTANT_VALUE',  -- Constant value
    NULL               -- Another NULL column
FROM temp_import;

-- Optional: Drop the temporary table when done
DROP TABLE temp_import;

--------
import logging
logging.basicConfig(filename='application.log', level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logging.info('Application started')


if not hasattr(cx_Oracle, '_oracle_client_initialized') or not cx_Oracle._oracle_client_initialized:
    cx_Oracle.init_oracle_client()


Concurrent Queries: 20 per account/region (default)
What happens when hit: New queries get queued or return TooManyRequestsException
Other key limits:

30 min max execution time
4 concurrent Spark sessions
100 databases per account

Solution: Request limit increase through AWS Support (can go up to 50-100+)
    cx_Oracle._oracle_client_initialized = True

**Append Mode:** Use for **incremental data ingestion** when you need to preserve complete **data lineage** and maintain **ACID compliance**. This mode implements **immutable storage patterns** where new records are added without modifying existing data, ensuring **zero data loss** and supporting **time-series analytics**. Perfect for **event streaming**, **CDC (Change Data Capture)** pipelines, **audit logs**, and **data lake bronze layers** where historical preservation is critical for **regulatory compliance** and **forensic analysis**.

**Overwrite Mode:** Use for **full refresh patterns** when implementing **dimension table reloads**, **snapshot replacements**, or **data quality remediation**. This mode performs **atomic operations** by completely replacing the target dataset, making it ideal for **reference data management**, **slowly changing dimensions (Type 1)**, **staging table refreshes**, and **ETL error recovery** scenarios. Critical for **data warehouse fact tables** and **master data management** where stale data must be completely purged and replaced with authoritative sources.

**Dynamic Save Mode:** Use for **partition-aware processing** in **big data architectures** where you need **selective data mutation** with **optimized I/O operations**. This mode leverages **predicate pushdown** and **partition pruning** to update only affected **Hive partitions** while maintaining **MVCC (Multi-Version Concurrency Control)**. Essential for **late-arriving data**, **slowly changing dimensions (Type 2)**, **partition-based upserts**, **data lake silver/gold layers**, and **Delta Lake merge operations** where you need **ACID transactions** with **schema evolution** support.

Append Mode: AWS Glue writes new Parquet files alongside existing files in your S3 table location. The Glue Data Catalog automatically registers new partitions and updates table metadata without affecting existing entries. Your S3 bucket grows with additional objects while preserving all historical data. Table queries can access both old and new data seamlessly, and partition discovery ensures new data becomes immediately queryable through services like Athena and Redshift Spectrum.
Overwrite Mode: AWS Glue performs a complete deletion of all existing S3 objects in the table path, then writes fresh Parquet files. The Glue Data Catalog replaces all existing partition metadata with new entries, effectively recreating the table structure. Your S3 storage is completely refreshed - old data disappears permanently and new data takes its place. This creates a clean slate but requires careful consideration since data recovery becomes impossible after execution.
Dynamic Save Mode: AWS Glue analyzes your partitioned table structure and updates only the S3 prefixes that contain changed data. The Glue Data Catalog selectively updates partition metadata for modified sections while preserving unchanged partitions. Some S3 objects get replaced while others remain untouched, creating an efficient update pattern. This approach optimizes both storage costs and query performance by minimizing unnecessary data movement and maintaining table history where appropriate.
